[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "Project4.html",
    "href": "Project4.html",
    "title": "Ethical Dilemmas of Risk Assessment Tools in Criminal Justice",
    "section": "",
    "text": "In a 2016 ProPublica article, Julia Angwin and her co-authors revealed that Northpointe’s COMPAS, a prominent risk assessment tool used to inform decisions in the criminal justice system, was racially biased. The tool failed at similar rates for white and Black defendants. However, it falsely labeled 44.9% of Black defendants as high risk who did not re-offend, compared to 23.5% of white defendants (2016). Actuarial risk assessment instruments (ARAIs) like COMPAS are increasingly being used across the criminal justice system. Its proponents argue they could reduce existing racial biases and ultimately decrease incarceration rates (Flores et al. 2016, 38), while detractors argue these untested and non-transparent tools risk amplifying biases against people of color.\n\nShould race be used as a variable?\n\nUsing race as a category is morally unacceptable in developing a risk assessment tool for use in the criminal justice system. However, even though race was not used in the COMPAS tool, certain metrics such as “substance abuse” and “residence/stability” might be correlated with racial categories, leading to bias in the model. For this reason, employing race as a variable is necessary for independent evaluations of tools like COMPAS. Only by documenting defendants’ race could Angwin and her team identify bias and ultimately pressure courts to reconsider their use of the tool.\n\nWere the data made publicly available? Why? How? On what platform?\n\nYes, the data were made publicly available on GitHub and are directly linked to the ProPublica article. This transparency allowed other scholars to verify and critique their findings, as evidenced by the 2016 article, “False Positives, False Negatives, and False Analyses.” Although not peer-reviewed and somewhat hyperbolic in its language, the article has been widely cited and identified key limitations in Angwin et al.’s analysis. These included collapsing a spectrum of predicted risk into two categories, falsely equating racial differences in mean scores with bias in the tool (rather than systemic racial bias), neglecting to utilize standardized methods to test for bias, and overstating their results (Flores et al. 2016, 39-40). Their own analysis found no evidence of racial bias in the COMPAS tool in terms of its accuracy in predicting recidivism rates (45). Only through publishing the original data were critiques like this made possible, thereby creating space for open debates over research methodologies, definitions of bias, and just uses of predictive modeling in criminal justice.\n\nWho was measured? Are those individuals representative of the people to whom we’d like to generalize / apply the algorithm? Should we analyze data if we do not know how the data were collected?\n\nAngwin et al. compiled a dataset of over 7,000 individuals arrested in Broward County, Florida, in 2013 and 2014, and determined whether they had committed new crimes in the subsequent two years (2016). The article’s text offers no specific reason for why Broward County was selected; however, according to the 2020 census, the county is home to significant populations of non-Hispanic whites (33.1%), non-Hispanic Blacks (26.6%), and Hispanics (31.3%). Because the purpose of the study was to analyze the bias of a predictive tool, it is perhaps less important that the sample population is closely representative of the US population as a whole. Nevertheless, the authors should explain why they selected Broward and why they chose to limit much of their analysis to white and Black populations. The authors were transparent about how they collected the data, and much (if not all) of the data is publicly available via court documentation and can therefore be verified.\n\nWhat was the consent structure for recruiting participants? Were the participants aware of the ways their data would be used for research? Was informed consent possible? Can you provide informed consent for applications that are yet foreseen?\n\nThe “participants” of the study were neither aware that their data would be used for research nor able to provide informed consent. Criminal records are already publicly available, and I don’t think the researchers violated any laws by conducting research using the data. However, by publishing the data on a highly popular and visible new article, they increase the probability that individuals in the defendants’ social networks will discover their criminal status, which the defendants would not have otherwise chosen to disclose.\nAngwin et al.’s results suggest that racial bias in risk assessment tools can harm Black people by causing them to serve longer sentences in jails and prisons even when they may be at lower risk of recidivism (2016). Flores et al. point out that the existing justice system is already biased against poor minorities due to economic factors, policing practices, prosecutor behavior, and judicial biases (2016, 38). In their view, the status quo is already biased, and risk assessment tools well-screened for racial biases could help reduce personal biases and human errors (38). I believe that both human and computer decision-making are subject to bias and must be scrutinized, made transparent, and held accountable. Risk assessment tools are particularly dangerous when they lack both transparency and rigorous testing and are then used at scale to cut costs and save time for judicial proceedings.\nSources:\nFlores, Anthony W, Kristin Bechtel, and Christopher Lowemkamp. 2016. “False Positives, False Negatives, and False Analyses: A Rejoinder to ‘Machine Bias: There’s Software Used Across the Country to Predict Future Criminals. And It’s Biased Against Blacks.’” Federal Probation 80: 2 https://heinonline.org/HOL/Page?handle=hein.journals/fedpro80&id=116&collection=journals&index=.\nAngwin Mattu, Julia, Jeff Larson, Lauren Kirchner, and Surya. 2016. “Machine Bias.” ProPublica, May 23. https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing."
  },
  {
    "objectID": "TidyTuesday2.html",
    "href": "TidyTuesday2.html",
    "title": "Race in obgyn studies",
    "section": "",
    "text": "Link to data: https://github.com/rfordatascience/tidytuesday/blob/main/data/2025/2025-02-25/readme.md\nOriginal data source: Lewis, Ayodele G. et al. “Racial and ethnic disparities in reproductive medicine in the United States: a narrative review of contemporary high-quality evidence” American Journal of Obstetrics & Gynecology, Volume 232, Issue 1, 82 - 91.e44.\nAbout my analysis\nThis data set compiled articles on racial/ethnic health disparaties from the 8 highest impact ob/gyn journals. I hoped to visualize how the language used to describe racial categories in these medical studies had changed over time. The first two graphs are examples of poor data visualization. The first graph has undifferentiated black lines illustrating how many times a certain term for a racial category was used. It is not easy to see that the vast majority of terms were not used at all for most years, and we can’t identify which line refers to which term. The second graph assigned color to each term but because there were so many categories, the legend obsures the entire graph."
  },
  {
    "objectID": "TidyTuesday2.html#so-in-the-third-graph-i-chose-to-filter-out-all-racial-categories-except-for-white-black-asian-and-pacific-islander-to-see-if-i-could-discern-a-pattern.-although-this-graph-sacrifices-a-lot-of-information-it-clearly-shows-that-white-and-black-populations-have-been-studied-more-extensively-than-asian-americans-and-pacific-islanders-have-at-least-in-a-disagreggated-way.",
    "href": "TidyTuesday2.html#so-in-the-third-graph-i-chose-to-filter-out-all-racial-categories-except-for-white-black-asian-and-pacific-islander-to-see-if-i-could-discern-a-pattern.-although-this-graph-sacrifices-a-lot-of-information-it-clearly-shows-that-white-and-black-populations-have-been-studied-more-extensively-than-asian-americans-and-pacific-islanders-have-at-least-in-a-disagreggated-way.",
    "title": "Race in obgyn studies",
    "section": "So, in the third graph, I chose to filter out all racial categories except for white, Black, Asian, and Pacific Islander to see if I could discern a pattern. Although this graph sacrifices a lot of information, it clearly shows that white and Black populations have been studied more extensively than Asian Americans and Pacific Islanders have, at least in a disagreggated way.",
    "text": "So, in the third graph, I chose to filter out all racial categories except for white, Black, Asian, and Pacific Islander to see if I could discern a pattern. Although this graph sacrifices a lot of information, it clearly shows that white and Black populations have been studied more extensively than Asian Americans and Pacific Islanders have, at least in a disagreggated way.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.3.0\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.1.0     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\ntuesdata &lt;- tidytuesdayR::tt_load('2025-02-25')\n\n---- Compiling #TidyTuesday Information for 2025-02-25 ----\n--- There are 2 files available ---\n\n\n── Downloading files ───────────────────────────────────────────────────────────\n\n  1 of 2: \"article_dat.csv\"\n  2 of 2: \"model_dat.csv\"\n\narticle_dat &lt;- tuesdata$article_dat\nmodel_dat &lt;- tuesdata$model_dat\n\nrace_language &lt;- article_dat |&gt;\n  select(study_year_end, race1, race2, race3, race4, race5, race6, race7, race8) |&gt;\n  pivot_longer(cols = race1:race8, names_to = \"RaceNumber\", values_to = \"race\") |&gt;\n  group_by(study_year_end, race) |&gt;\n  summarize(count=n()) |&gt;\n  filter(is.na(race) == FALSE) |&gt;\n  filter(study_year_end!=-99)\n\n`summarise()` has grouped output by 'study_year_end'. You can override using\nthe `.groups` argument.\n\nggplot(data = race_language, aes(x=study_year_end, y=count, group=race)) + geom_line() +\n  labs(\n    title = \"Racial groups mentioned in ob-gyn studies over time (without color)\",\n    x = \"Year\",\n    y = \"Number of mentions\"\n  ) \n\n\n\n\n\n\n\nggplot(data = race_language, aes(x=study_year_end, y=count, color=race)) + geom_line() +\n  labs(\n    title = \"Racial groups mentioned in ob-gyn studies (with color)\",\n    x = \"Year\",\n    y = \"Number of mentions\"\n  ) \n\n\n\n\n\n\n\n\n\nlibrary(tidyverse)\n\ntuesdata &lt;- tidytuesdayR::tt_load('2025-02-25')\n\n---- Compiling #TidyTuesday Information for 2025-02-25 ----\n--- There are 2 files available ---\n\n\n── Downloading files ───────────────────────────────────────────────────────────\n\n  1 of 2: \"article_dat.csv\"\n  2 of 2: \"model_dat.csv\"\n\narticle_dat &lt;- tuesdata$article_dat\nmodel_dat &lt;- tuesdata$model_dat\n\nrace_language &lt;- article_dat |&gt;\n  select(study_year_end, race1, race2, race3, race4, race5, race6, race7, race8) |&gt;\n  pivot_longer(cols = race1:race8, names_to = \"RaceNumber\", values_to = \"race\") |&gt;\n  group_by(study_year_end, race) |&gt;\n  summarize(count=n()) |&gt;\n  filter(is.na(race) == FALSE) |&gt;\n  filter(study_year_end!=-99) |&gt;\n  filter(race == \"White\" | race == \"Black\" | race == \"Pacific Islander\" | race == \"Asian American\")\n\n`summarise()` has grouped output by 'study_year_end'. You can override using\nthe `.groups` argument.\n\nggplot(data = race_language, aes(x=study_year_end, y=count, color=race)) + geom_line() +\n  labs(\n    title = \"Racial groups mentioned in oby-gyn studies (including only certain racial groups)\",\n    x = \"Year\",\n    y = \"Number of mentions\"\n  )"
  },
  {
    "objectID": "Project2.html",
    "href": "Project2.html",
    "title": "FLOTUS Tweets",
    "section": "",
    "text": "Data source: “Archived White House Websites and Social Media.” 2017. Barack Obama Presidential Library. Accessed October 4, 2025. https://www.obamalibrary.gov/digital-research-room/archived-white-house-websites-and-social-media.\nI analyzed data on Michelle Obama’s tweets from 2013 to 2016 to answer three questions: 1) What were her ten most common hashtags and how many times did she use them? 2) From which ten accounts did she retweet most frequently and how many times did she retweet for each? 3) How did her usage of words central to her policy platform—girls, health, and education—change from year to year?\n\n\nShow the code\nlibrary(tidyverse)\nlibrary(lubridate)\n\nFlotusTweets &lt;- read.csv(\"tweets.csv\")\n\nFlotusHashtags &lt;- FlotusTweets|&gt;\n  select(tweet_id, text) |&gt;\n  mutate(hashtags = str_extract_all(text, \"(?&lt;=#)\\\\w+\"))\n\nHashtagsUnnested &lt;- FlotusHashtags |&gt;\n  unnest(hashtags)\n\nHashtagCounts &lt;- HashtagsUnnested |&gt;\n  count(hashtags, sort = TRUE) |&gt;\n  arrange(desc(n)) |&gt;\n  head(10) |&gt;\n  mutate(hashtags = fct_reorder(hashtags, n))\n\nggplot(data=HashtagCounts, aes(y=hashtags, x=n, fill=hashtags)) +\n  scale_fill_manual(values = c(rep(\"grey\", 9), \"pink\")) +\n  geom_col(show.legend = FALSE) + \n  labs(title = \"Michelle Obama's Top 10 Hashtags\", y=\"Hashtag\", x=\"Number of uses\")\n\n\n\n\n\n\n\n\n\nAs you can see from the graph above, Michelle Obama’s most commonly used hashtag was #LetGirlsLearn, followed by #ReachHigher, #GimmeFive, #62MillionGirls, and #LetsMove. Although her use of hashtags likely does not map on perfectly to her priorities, it provides some indication of the social media campaigns she was promoting most actively.\n\n\nShow the code\nFlotusRetweets &lt;- FlotusTweets|&gt;\n  select(tweet_id, timestamp, text) |&gt;\n  mutate(retweets = str_extract(text, \"(?&lt;=RT @)\\\\w+\")) |&gt;\n  filter(!is.na(retweets))\n\nRetweetsUnnested &lt;- FlotusRetweets |&gt;\n  unnest(retweets)\n\nRetweetCounts &lt;- RetweetsUnnested |&gt;\n  count(retweets, sort = TRUE) |&gt;\n  arrange(desc(n)) |&gt;\n  head(10) |&gt;\n  mutate(retweets = fct_reorder(retweets, n))\n\nggplot(data=RetweetCounts, aes(y=retweets, x=n, fill=retweets)) +\n  scale_fill_manual(values = c(rep(\"grey\", 9), \"light blue\")) +\n  geom_col(show.legend = FALSE) + \n  labs(title = \"Michelle Obama's Top 10 Accounts for Retweets\", y=\"Accounts\", x=\"Number of retweets\")\n\n\n\n\n\n\n\n\n\nMichelle Obama retweeted from a number of accounts, especially the Let’s Move Campaign, the White House, the Joining Forces Campaign (an initiative with Jill Biden to support veterans), and the Reach Higher Campaign (for educational access). Clearly, she was promoting a variety of initiatives relevant to her platform as first lady.\n\n\nShow the code\nFlotusTopics &lt;- FlotusTweets|&gt;\n  select(tweet_id, timestamp, text) |&gt;\n  mutate(girls = str_detect(text, \"girls*\")) |&gt;\n  mutate(education = str_detect(text, \"education\")) |&gt;\n  mutate(health = str_detect(text, \"health\")) |&gt;\n  mutate(year = year(timestamp)) |&gt;\n  select(year, girls, education, health) |&gt;\n  group_by(year)|&gt;\n  summarize(across(c(girls, education, health), sum)) |&gt;\n  pivot_longer(-year, names_to=\"Topics\", values_to=\"mentions\")\n\nggplot(data=FlotusTopics, aes(y=mentions, x=year, color=Topics)) +\n   geom_line() + \n   labs(title = \"Michelle Obama's Tweets on Girls, Education, and Health over Time\", y=\"Tweets\", x=\"Year\")\n\n\n\n\n\n\n\n\n\nMichelle Obama used her platform as First Lady to promote education, health, and opportunities for girls. I wanted to understand how her usage of these terms on Twitter changed over the course of her time in the White House. Although her total amount of tweets across all three terms increased from 2013 to 2014 and from 2014 to 2015, her usage of the term girl spiked particularly dramatically in 2015. She used all three terms less frequently in 2016."
  },
  {
    "objectID": "Project3.html",
    "href": "Project3.html",
    "title": "Gender Representation in the US Congress",
    "section": "",
    "text": "Data source: Zachary Kornbluth. 2025. Unitedstates/Congress-Legislators. Python. September 27, Released October 22, 2025. https://github.com/unitedstates/congress-legislators.\nThere are currently 151 female members in the US Congress, out of 535 voting members. The distribution of women across parties is uneven: 112 are part of the Democratic Party, and the other 43 are part of the Republican Party. Assuming that women are equally likely to be a part of each party, how likely is it that number of Democratic women is 112 are more? How likely is it that different levels of female representation is related to the party rather than random chance?\nTo answer these questions, I first wrangled data on the current members of Congress to calculate the numbers of men and women in each of the two main parties. Next, I created a data frame with the same numbers of Democrats, Republicans, women, and men, and then created a function that randomly assigns parties to the men and women in the data frame and then calculates the total number of Democratic women. Finally, I ran this function 5000 times using the map_dbl function and then calculated the number of times that Democratic women exceeded 112 divided by the number of iterations (5000). This figure denotes the probability that the number of Democratic women reached or exceeded 112 when the assignment of women to each party was random.\n\n\nShow the code\nlibrary(reprex)\nset.seed(47)\n\nlibrary(tidyverse)\nlibrary(readr)\nurl &lt;- \"https://raw.githubusercontent.com/unitedstates/congress-legislators/gh-pages/legislators-current.csv\"\ncurrent_legislators &lt;- read_csv(url)\n\nrep_and_dem_women &lt;- current_legislators |&gt; \n  group_by(party, gender) |&gt;\n  summarize(count = n(), .groups = \"drop\") |&gt;\n  group_by(party) |&gt;\n  mutate(proportion_gender_by_party = count/sum(count)) |&gt;\n  filter(party == \"Democrat\" | party == \"Republican\")\n\nstart_data &lt;- data.frame(gender = c(rep(\"F\", 155), rep(\"M\", 381)),\n                         party = c(rep(\"Democrat\", 261), rep(\"Republican\", 275)))\n\nrandom_female_democrats &lt;- function(rep) {\n  start_data |&gt; mutate(perm_party = sample(party)) |&gt;\n    #select(gender, perm_party) |&gt;\n    group_by(gender, perm_party) |&gt;\n    summarize(count = n(), .groups = \"drop\") |&gt;\n    filter(perm_party == \"Democrat\", gender == \"F\") |&gt;\n    select(count) |&gt;\n    pull()\n}\n\nnum_exper &lt;- 5000\nfemale_democrats &lt;- map_dbl(1:num_exper, random_female_democrats)\n\nsum(female_democrats &gt;= 112) / num_exper\n\n\n[1] 0\n\n\nShow the code\nfemale_democrats |&gt;\n   data.frame() |&gt;\n   ggplot(aes(x = female_democrats)) +\n   geom_histogram() +\n   geom_vline(xintercept = 112, color = \"red\") +\n   labs(x = \"Number of women in Democratic seats in Congress\",\n        title = \"Numbers of Democratic women when party assignments are random\",\n        subtitle = \"Higher female representation in the Democratic party is unlikely due to random chance.\")\n\n\n\n\n\n\n\n\n\nAs you can see from the chart, the actual number of Democratic women in congressional seats far exceeds the number of women in Democratic seats when the assignment of party to women and men is random. As expected, the random distribution of Democratic women centers around 77.5, or half the total women in Congress.\nI calculated that when women are randomly assigned to Democratic and Republican seats 5000 times, the probability that 112 or more were in the Democratic Party was 0. This result suggests the higher number of women in the Democratic Party, in comparison to the Republican Party, has something to do with the parties and not random chance. Future research should determine the specific mechanisms facilitating greater female participation in the Democratic Party."
  }
]