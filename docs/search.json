[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Abigail Krenz",
    "section": "",
    "text": "Hello there! I study international relations and Middle Eastern studies at Pomona College."
  },
  {
    "objectID": "Project5.html",
    "href": "Project5.html",
    "title": "Traffic Stops in Bay Area Cities",
    "section": "",
    "text": "Using data from the Stanford Open Policing database, this project aimed to analyze traffic stops across three Bay Area cities: San Jose, San Francisco, and Oakland. Specifically, I was curious how the volume of traffic stops, and pedestrian stops specifically, changed over time. I also wanted to determine whether there was a relationship between the type of stop (vehicular or pedestrian) and the stop’s outcome. I first opened a connection with the Stanford Open Policing database:\n\n\nShow the code\nlibrary(tidyverse)\ncon_traffic &lt;- DBI::dbConnect(\n  RMariaDB::MariaDB(),\n  dbname = \"traffic\",\n  host = Sys.getenv(\"TRAFFIC_HOST\"),\n  user = Sys.getenv(\"TRAFFIC_USER\"),\n  password = Sys.getenv(\"TRAFFIC_PWD\")\n)\n\n\nI then queried the tables of the three cities to find the total number of traffic stops made each month:\n\n\nShow the code\nSELECT \"Oakland\" AS city, DATE_FORMAT(date, '%Y-%m') AS yr_month, COUNT(*) AS num_stops\n      FROM ca_oakland_2020_04_01\n      GROUP BY yr_month\n      \nUNION\n\nSELECT \"San Francisco\" AS city, DATE_FORMAT(date, '%Y-%m') AS yr_month, COUNT(*) AS num_stops\n      FROM ca_san_francisco_2020_04_01\n      GROUP BY yr_month\n\nUNION\n\nSELECT \"San Jose\" as city, DATE_FORMAT(date, '%Y-%m') AS yr_month, COUNT(*) AS num_stops\n      FROM ca_san_jose_2020_04_01\n      GROUP BY yr_month;\n\n\nNext, I charted these monthly traffic stops in a line graph. As you can see below, the time frame of available data differs across each city: In San Francisco, data began in 2007 and ended in mid-2026. For Oakland and San Jose, data began in 2013 and ended around 2018. San Francisco has consistently seen the highest numbers of stops, while the number of stops in Oakland and San Jose have been less, and roughly comparable to one another.\n\n\nShow the code\nstops_by_month |&gt;\n  mutate(yr_month = as.Date(str_c(yr_month, \"-01\"))) |&gt;\n  ggplot(aes(x = yr_month, y = num_stops, color = city)) +\n    geom_line() +\n    labs(title = \"Traffic Stops in Bay Area Cities by Month\", \n         x = \"Year\", \n         y = \"Traffic Stops\") +\n  theme_minimal()\n\n\n\n\n\nIn San Francisco from 2007 to 2014, monthly traffic stops ranged from around 4,000 to 10,500, seeing a slight decrease from 2010 to 2014. In Oakland from 2013 to 2018, monthly stops ranged from 4,000 to 1,500, with the exception of a spike in October 2013, when stops nearly reached 6,000. In San Jose from 2013 to 2018, stops ranged from 1,500 to 4,000.\n\n\n\n\nI then followed a similar process to examine the number of pedestrian stops made in the three cities over time.\n\n\nShow the code\nSELECT \"Oakland\" AS city, DATE_FORMAT(date, '%Y-%m') AS yr_month, SUM(type = \"pedestrian\") AS ped_stops\n      FROM ca_oakland_2020_04_01\n      GROUP BY yr_month\n      \nUNION\n\nSELECT \"San Francisco\" AS city, DATE_FORMAT(date, '%Y-%m') AS yr_month, SUM(type = \"pedestrian\") AS ped_stops\n      FROM ca_san_francisco_2020_04_01\n      GROUP BY yr_month\n\nUNION\n\nSELECT \"San Jose\" as city, DATE_FORMAT(date, '%Y-%m') AS yr_month, SUM(type = \"pedestrian\") AS ped_stops\n      FROM ca_san_jose_2020_04_01\n      GROUP BY yr_month\n\n\nAs the following line graph reveals, even though the San Francisco data included a variable for the type of stop, no pedestrian stops were documented across the period. Oakland seems to have only begun documenting pedestrian stops in late 2015. In San Jose from 2013 to 2018, the numbers of pedestrian stops decreased, in Oakland from 2016 to 2018, the number of pedestrian stops did not noticeably increase or decrease.\n\n\nShow the code\nped_stops_over_time |&gt;\n  mutate(yr_month = as.Date(str_c(yr_month, \"-01\"))) |&gt;\n  ggplot(aes(x = yr_month, y = ped_stops, color = city)) +\n    geom_line(alpha = 0.8) +\n    labs(title = \"Pedestrian Traffic Stops in Bay Area Cities Over Time\", \n         x = \"Year\", \n         y = \"Traffic Stops\") +\n  theme_minimal()\n\n\n\n\n\nn San Francisco from 2007 to 2014, there were zero pedestrian stops. In Oakland from 2013 to 2018, monthly stops were zero until mid-2015 and thereafter ranged from 200 to 200. In San Jose from mid-2013 to mid-2018, pedestrian stops peaked at over 1500 in October 2013 and generally descreased until mid-2016 when it seemed to stabilize around 500 stops.\n\n\n\n\nThe following SQL chunk confirmed my suspicion that San Francisco only documented vehicular stops:\n\n\nShow the code\nSELECT DISTINCT type FROM ca_san_francisco_2020_04_01;\n\n\n\n1 records\n\n\ntype\n\n\n\n\nvehicular\n\n\n\n\n\nTo examine what relationship existed between the type of stop and its outcome (arrest, citation, etc.) across cities, I queried and graphed the data for all three cities.\nSan Francisco\nPredictably, the graph for San Francisco was not very interesting because only one type of arrest existed in the data. Citations were by far the most common outcome followed by warnings. Only small numbers of stops ended in arrests, and only a tiny proportion of stops had outcomes that were not documented.\n\n\nShow the code\nSELECT outcome, type, COUNT(*) AS count\nFROM ca_san_francisco_2020_04_01\nGROUP BY type, outcome;\n\n\n\n\nShow the code\n#Bar graph\noutcomes_by_type |&gt;\n  ggplot(aes(x = type, y = count, fill = outcome)) +\n      geom_bar(stat = \"identity\") +\n      labs(title = \"Traffic Stops in San Francisco by Type and Outcome, 2007-2016\", \n           fill = \"Outcome of Stop\", \n           x = \"Type of Stop\") +\n  theme_minimal()\n\n\n\n\n\nAmong vehicular stops in San Francisco, citations were by far the most common outcome followed by warnings. Only small numbers of stops ended in arrests, and only a tiny proportion of stops had outcomes that were not documented.\n\n\n\n\nSan Jose\nIn San Jose, it was quite common for the outcome of a stop to be missing, especially for pedestrian stops, and no “warning” outcome was included. The type of stop was also missing for many observations. Arrests were more common for pedestrian stops as opposed to vehicular ones. Conversely, citations were more common for vehicular stops than for pedestrian ones.\n\n\nShow the code\nSELECT outcome, type, COUNT(*) AS count\nFROM ca_san_jose_2020_04_01\nGROUP BY type, outcome;\n\n\n\n\nShow the code\n#Bar graph\noutcomes_by_type_sj |&gt;\n  ggplot(aes(x = type, y = count, fill = outcome)) +\n      geom_bar(stat = \"identity\") +\n      scale_fill_manual(values = c(\"citation\" = \"#00BB38\", \"arrest\" = \"#F7766D\", \"NA\" = \"#7F7F7F\")) +\n      labs(title = \"Traffic Stops in San Jose by Type and Outcome, 2013-2018\", \n           fill = \"Outcome of Stop\", \n           x = \"Type of Stop\") +\n  theme_minimal()\n\n\n\n\n\nIn San Jose, most stops were vehicular, among which over half had no documented outcome, many ended in citation, and a very small number ended in arrests. Among pedestrian stops, over 3/4 had no documented outcome, a comparatively higher percentage ended in arrest, and comparatively fewer ended in citation. A significant number of stops were not categorized as vehicular or pedestrian.\n\n\n\n\nOakland\nIn Oakland, outcomes included a “warning category.” Although pedestrian stops were rare, a higher percentage of percentage of pedestrian stops ended in arrest in comparison to vehicular stops. Both warnings and citations were more common for vehicular stops.\n\n\nShow the code\nSELECT outcome, type, COUNT(*) AS count\nFROM ca_oakland_2020_04_01\nGROUP BY type, outcome;\n\n\n\n\nShow the code\n#Bar graph\noutcomes_by_type_oak |&gt;\n  ggplot(aes(x = type, y = count, fill = outcome)) +\n      geom_bar(stat = \"identity\") +\n      labs(title = \"Traffic Stops in Oakland by Type and Outcome, 2013-2018\", \n           fill = \"Outcome of Stop\",\n           x = \"Type of Stop\") +\n  theme_minimal()\n\n\n\n\n\nIn Oakland, most stops were vehicular, among which less than a quarter had no documented outcome, many ended in citations (49410) and warnings (28146), and a small number ended in arrests (6693). Among the small number of pedestrian stops, there were 2855 arrests, 565 citations, 1174 warnings, and 2490 with no documented outcome. Among the significant number of stops not categorized as vehicular or pedestrian, 9609 has no outcome, 6642 ended in arrest, 2451 ended in citation, and 1364 ended in a warning.\n\n\n\n\nConclusion\nThis analysis revealed the inconsistent, and often incomplete, ways that traffic stops have been documented in these three Bay Area cities across time. Pedestrian stops were not documented at all in San Francisco and were only documented after the first two years of available data in Oakland. In both San Jose and Oakland, information on the type and outcome was missing for a high proportion of traffic stops. Nevertheless, preliminary analysis revealed that for both San Jose and Oakland, arrests were more common for pedestrian stops than vehicular ones, and citations were more common for vehicular stops than pedestrian ones. This pattern may exist because the police are more likely to stop cars for small infractions (running stop signs, out-dated registration, broken headlight, etc) and more likely to stop individuals for more serious criminal behavior warranting arrest (violence, drug dealing, etc.).\nData source: Pierson, Emma, Camelia Simoiu, Jan Overgoor, Sam Corbett-Davies, Daniel Jenson, Amy Shoemaker, Vignesh Ramachandran, et al. 2020. “A Large-Scale Analysis of Racial Disparities in Police Stops Across the United States.” Nature Human Behaviour, 1–10. https://pubmed.ncbi.nlm.nih.gov/32367028/."
  },
  {
    "objectID": "TidyTuesday2.html",
    "href": "TidyTuesday2.html",
    "title": "Race in OB/GYN studies",
    "section": "",
    "text": "Drawing from a data set of articles on racial and ethnic health disparities from the eight highest-impact OB/GYN journals, I hoped to visualize how the language used to describe racial categories in these medical studies had changed over time. In the process, I encountered some instructive challenges in data wrangling. As you can already see in the graph above (and repeated below), researchers used an extraordinary variety of terms to describe racial categories, making it difficult to discern patterns in their use.\nI began by identifying all racial categories mentioned in the articles and the number of times they appeared. You can see the first ten lines of the wrangled data frame below:\n\n\nShow the code\nlibrary(tidyverse)\nlibrary(plotly)\n\n#Loads data\ntuesdata &lt;- tidytuesdayR::tt_load('2025-02-25')\narticle_dat &lt;- tuesdata$article_dat\n\n#Wrangles racial categories and adds up the number of articles in which they appeared.\nrace_language &lt;- article_dat |&gt;\n  select(study_year_end, race1, race2, race3, race4, race5, race6, race7, race8) |&gt;\n  pivot_longer(cols = race1:race8, names_to = \"RaceNumber\", values_to = \"race\") |&gt;\n  mutate(race = str_to_lower(race)) |&gt;\n  group_by(study_year_end, race) |&gt;\n  summarize(count=n()) |&gt;\n  filter(is.na(race) == FALSE) |&gt;\n  filter(study_year_end!=-99)\n\n#Displays first three rows\nhead(race_language, 10)\n\n\n# A tibble: 10 × 3\n# Groups:   study_year_end [2]\n   study_year_end race                   count\n            &lt;dbl&gt; &lt;chr&gt;                  &lt;int&gt;\n 1           1998 american indian            1\n 2           1998 asian/pacific islander     1\n 3           1998 black                      1\n 4           1998 hispanic                   1\n 5           1998 unknown                    1\n 6           1998 white                      1\n 7           2001 black                      1\n 8           2001 hispanic                   1\n 9           2001 other                      1\n10           2001 white                      1\n\n\nNext, I created a basic line graph of the illustrating how many times a certain term for a racial category was used. Because it uses undifferentiated black lines, it is not easy to see that most terms were not used at all for most years. Additionally, it is not possible to identify which line refers to which term.\n\n\nShow the code\n#Line graph (without color)\nggplot(data = race_language, aes(x=study_year_end, y=count, group=race)) + geom_line() +\n  labs(\n    title = \"Racial groups mentioned in OB/GYN studies over time (without color coding)\",\n    x = \"Year\",\n    y = \"Number of mentions\"\n  ) +\n  theme_minimal()\n\n\n\n\n\nThere is no discernable pattern or trend across the time period, and many terms are mentioned only once. The undifferentiated black lines make identifying specific terms impossible.\n\n\n\n\nHowever, when I assigned a specific color to each term in the graph below, there were so many categories that the legend obscured the entire graph.\n\n\nShow the code\n#Line graph with color\nggplot(data = race_language, \n       aes(x=study_year_end, y=count, color=race)) + \n  geom_line() +\n  labs(\n    title = \"Racial groups mentioned in OB/GYN studies (with color coding)\",\n    x = \"Year\",\n    y = \"Number of mentions\"\n  ) +\n  theme_minimal()\n\n\n\n\n\nThere are so many racial categories that the legend takes up the entire space.\n\n\n\n\nAdding interactive hover labeling in the graph below through plotly partially addressed this problem by allowing the viewer to more easily identify which line referred to which term.\n\n\nShow the code\nplot &lt;- ggplot(data = race_language, \n               aes(x=study_year_end, y=count, color=race, text = paste('Race:', race))) + \n  geom_line() +\n  labs(\n    title = \"Racial groups mentioned in OB/GYN studies over time (with interactive labeling)\",\n    x = \"Year\",\n    y = \"Number of mentions\",\n    color = \"Race\"\n  ) +\n  theme_minimal()\n\n#Hover labelling\nggplotly(plot, tooltip = \"text\")\n\n\n\n\nInteractive hover labelling makes identifying specific racial categories on the graph easier, but with so many lines are overlapping, it is still difficult to see all the categories.\n\n\nFinally, I chose to filter out all racial categories except for white, Black, Asian, Hispanic, and Pacific Islander to see if I could discern a pattern in the use of these terms over time.\n\n\nShow the code\n#Filtered for certain racial categories\nrace_language2 &lt;- race_language |&gt;\n  filter(race == \"white\" | \n           race == \"black\" | \n           race == \"pacific islander\" | \n           race == \"asian\" | \n           race == \"asian/pacific islander\" | \n           race == \"hispanic\")\n\n#Line graph\nggplot(data = race_language2, \n       aes(x=study_year_end, y=count, color=race)) + \n  geom_line() +\n  labs(\n    title = \"Racial groups mentioned in OB/GYN studies (including only certain racial groups)\",\n    x = \"Year\",\n    y = \"Number of mentions\",\n    color = \"Race\"\n  ) +\n  theme_minimal()\n\n\n\n\n\nWhite, Black, and Hispanic were mentioned a fairly comparable amount, Asian and Asian/Pacific Islander were mentioned less, and Pacific Islander was not mentioned at all.\n\n\n\n\nAlthough this graph sacrifices information, it shows that white, Black, and Hispanic populations have been studied more extensively than Asian Americans and Pacific Islander populations have been. Indeed, Pacific Islanders were not mentioned at all as a singular group. This finding suggests that researchers should prioritize more disaggregated studies of Pacific Islanders to better understand their unique reproductive health needs. The high level of year-to-year variation makes it difficult to discern whether there was any particular trend in the mentions of these terms over time.\nConclusion:\nNone of these graphs clearly captured all of the information due to the inconsistency in how racial groups were described and categorized across OB/GYN studies. Although this diversity is perhaps to be expected, it makes it more difficult for researchers to identify and compile health research focused on a specific racial group.\nData: https://github.com/rfordatascience/tidytuesday/blob/main/data/2025/2025-02-25/readme.md\nOriginal source: Lewis, Ayodele G. et al. “Racial and ethnic disparities in reproductive medicine in the United States: a narrative review of contemporary high-quality evidence” American Journal of Obstetrics & Gynecology, Volume 232, Issue 1, 82 - 91.e44. https://pubmed.ncbi.nlm.nih.gov/39059596/."
  },
  {
    "objectID": "TidyTuesday1.html",
    "href": "TidyTuesday1.html",
    "title": "Inequality across countries",
    "section": "",
    "text": "Using a World Bank data set, I visualized the inequality, as measured by the Gini coefficient, of 52 countries across the last few decades. Historically, South Africa has experienced the highest levels of inequality, followed by Brazil, although inequality in both countries has decreased over time. Countries with low levels of inequality included Iceland, Finland, and Switzerland.\n\n\nShow the code\nlibrary(tidyverse)\nlibrary(plotly)\n\n#Reads data\ntuesdata &lt;- tidytuesdayR::tt_load('2025-08-05')\nincome_inequality_processed &lt;- tuesdata$income_inequality_processed\nincome_inequality_raw &lt;- tuesdata$income_inequality_raw\n\nplot &lt;-ggplot(data=income_inequality_processed, \n              aes(x=Year, \n                  y=gini_mi_eq, \n                  color=Entity, \n                  text = paste('Country:', Entity))) +\n  geom_line() + \n  labs(title=\"Inequality in countries across time\", x=\"Year\", y=\"Gini Coefficient\", color=\"Country\", caption = \"Hello\") + \n  theme_minimal()\n\n#Uses ggplotly to add interactive hover labeling\nggplotly(plot, tooltip = \"text\")\n\n\n\n\nCountry with the highest inequality by far was South Africa. Those with low inequality included Iceland, Finland, and Switzerland.\n\n\nData: https://github.com/rfordatascience/tidytuesday/blob/main/data/2025/2025-08-05/readme.md\nOriginal source: Joe Hasell, Bertha Rohenkohl, Pablo Arriagada, Esteban Ortiz-Ospina, and Max Roser (2023) - “Economic Inequality” Published online at OurWorldinData.org. Retrieved from: ‘https://ourworldindata.org/economic-inequality’ [Online Resource]"
  },
  {
    "objectID": "Presentation.html#introduction",
    "href": "Presentation.html#introduction",
    "title": "DS Final Presentation",
    "section": "Introduction",
    "text": "Introduction\nData Source:\n“Archived White House Websites and Social Media.” 2017. Barack Obama Presidential Library. Accessed October 4, 2025. https://www.obamalibrary.gov/digital-research-room/archived-white-house-websites-and-social-media.\nResearch questions:\n\nWhat were Michelle Obama’s top ten hash tags, and how many times did she use them?\nFrom which ten accounts, did Michelle Obama re-tweet most frequently, and how many times did she re-tweet from each?\nHow did her usage of words central to her policy platform—girls, health, and education—change from year to year?"
  },
  {
    "objectID": "Presentation.html#introducing-the-data-set",
    "href": "Presentation.html#introducing-the-data-set",
    "title": "DS Final Presentation",
    "section": "Introducing the data set",
    "text": "Introducing the data set\n\nlibrary(tidyverse)\nlibrary(lubridate)\n\nflotus_tweets &lt;- read.csv(\"tweets.csv\")\n\nnames(flotus_tweets)\n\n [1] \"tweet_id\"                   \"in_reply_to_status_id\"     \n [3] \"in_reply_to_user_id\"        \"timestamp\"                 \n [5] \"source\"                     \"text\"                      \n [7] \"retweeted_status_id\"        \"retweeted_status_user_id\"  \n [9] \"retweeted_status_timestamp\" \"expanded_urls\""
  },
  {
    "objectID": "Presentation.html#tweeting-volume-over-time",
    "href": "Presentation.html#tweeting-volume-over-time",
    "title": "DS Final Presentation",
    "section": "Tweeting volume over time",
    "text": "Tweeting volume over time"
  },
  {
    "objectID": "Presentation.html#hashtags",
    "href": "Presentation.html#hashtags",
    "title": "DS Final Presentation",
    "section": "Hashtags",
    "text": "Hashtags"
  },
  {
    "objectID": "Presentation.html#retweets",
    "href": "Presentation.html#retweets",
    "title": "DS Final Presentation",
    "section": "Retweets",
    "text": "Retweets"
  },
  {
    "objectID": "Presentation.html#issue-areas",
    "href": "Presentation.html#issue-areas",
    "title": "DS Final Presentation",
    "section": "Issue areas",
    "text": "Issue areas"
  },
  {
    "objectID": "Presentation.html#conclusion",
    "href": "Presentation.html#conclusion",
    "title": "DS Final Presentation",
    "section": "Conclusion",
    "text": "Conclusion\n\nUse of regular expressions to extract data\nMore active social media use during the mid-years of Obama’s second term"
  },
  {
    "objectID": "Presentation.html#introduction-1",
    "href": "Presentation.html#introduction-1",
    "title": "DS Final Presentation",
    "section": "Introduction",
    "text": "Introduction\nData:\nPierson, Emma, Camelia Simoiu, Jan Overgoor, Sam Corbett-Davies, Daniel Jenson, Amy Shoemaker, Vignesh Ramachandran, et al. 2020. “A Large-Scale Analysis of Racial Disparities in Police Stops Across the United States.” Nature Human Behaviour, 1–10. https://pubmed.ncbi.nlm.nih.gov/32367028/.\nResearch questions:\n\nHow did the volume of traffic stops (and pedestrian stops specifically) change over time in San Jose, San Francisco, and Oakland?\nDoes a relationship exist between the type of stop (vehicular or pedestrian) and the stop’s outcome in these three Bay Area cities?"
  },
  {
    "objectID": "Presentation.html#traffic-stops-by-month",
    "href": "Presentation.html#traffic-stops-by-month",
    "title": "DS Final Presentation",
    "section": "Traffic Stops by Month",
    "text": "Traffic Stops by Month"
  },
  {
    "objectID": "Presentation.html#pedestrian-stops-by-month",
    "href": "Presentation.html#pedestrian-stops-by-month",
    "title": "DS Final Presentation",
    "section": "Pedestrian stops by month",
    "text": "Pedestrian stops by month"
  },
  {
    "objectID": "Presentation.html#what-happened-with-pedestrian-stops-in-san-francisco",
    "href": "Presentation.html#what-happened-with-pedestrian-stops-in-san-francisco",
    "title": "DS Final Presentation",
    "section": "What happened with pedestrian stops in San Francisco?",
    "text": "What happened with pedestrian stops in San Francisco?\n\nSELECT DISTINCT type FROM ca_san_francisco_2020_04_01;\n\n\n1 records\n\n\ntype\n\n\n\n\nvehicular"
  },
  {
    "objectID": "Presentation.html#investigating-type-of-stop-and-outcome",
    "href": "Presentation.html#investigating-type-of-stop-and-outcome",
    "title": "DS Final Presentation",
    "section": "Investigating Type of Stop and Outcome",
    "text": "Investigating Type of Stop and Outcome\nTo examine what relationship existed between the type of stop and its outcome (arrest, citation, etc.) across cities, I queried and graphed the data for all three cities."
  },
  {
    "objectID": "Presentation.html#san-francisco",
    "href": "Presentation.html#san-francisco",
    "title": "DS Final Presentation",
    "section": "San Francisco",
    "text": "San Francisco"
  },
  {
    "objectID": "Presentation.html#san-jose",
    "href": "Presentation.html#san-jose",
    "title": "DS Final Presentation",
    "section": "San Jose",
    "text": "San Jose"
  },
  {
    "objectID": "Presentation.html#oakland",
    "href": "Presentation.html#oakland",
    "title": "DS Final Presentation",
    "section": "Oakland",
    "text": "Oakland"
  },
  {
    "objectID": "Presentation.html#conclusion-1",
    "href": "Presentation.html#conclusion-1",
    "title": "DS Final Presentation",
    "section": "Conclusion",
    "text": "Conclusion\n\nTraffic stops have been documented in incomplete and inconsistent ways in these three Bay Area cities across time\nFor both San Jose and Oakland, arrests were more common for pedestrian stops than vehicular ones, and citations were more common for vehicular stops than pedestrian ones"
  },
  {
    "objectID": "Project4.html",
    "href": "Project4.html",
    "title": "Ethical Dilemmas of Risk Assessment Tools in Criminal Justice",
    "section": "",
    "text": "In a 2016 ProPublica article, Julia Angwin and her co-authors revealed that Northpointe’s COMPAS, a prominent risk assessment tool used to inform decisions in the criminal justice system, was racially biased. While the tool failed at similar rates for white and Black defendants, it falsely labeled 44.9% of Black defendants as high risk who did not re-offend, as compared to 23.5% of white defendants (2016). Actuarial risk assessment instruments (ARAIs) like COMPAS are increasingly being used across the criminal justice system. Its proponents argue they could reduce existing racial biases and ultimately decrease incarceration rates (Flores et al. 2016, 38), while detractors argue these untested and non-transparent tools risk amplifying biases against people of color.\nUse of race as a variable in COMPAS tool and follow-up studies\nUsing race as a category is morally unacceptable in developing a risk assessment tool for use in the criminal justice system. However, even though race was not used in the COMPAS tool, certain metrics such as “substance abuse” and “residence/stability” might be correlated with racial categories, leading to bias in the model. For this reason, employing race as a variable is necessary for independent evaluations of tools like COMPAS. Only by documenting defendants’ race could Angwin and her team identify bias and ultimately pressure courts to reconsider their use of the tool.\nData transparency in COMPAS and follow-up studies\nThe COMPAS algorithm itself and the data used to create it were not made public. In their follow-up investigation, however, Angwin and her team published their data on GitHub and added a link to the data to their ProPublica article. This transparency allowed other scholars to verify and critique their findings, as evidenced by the 2016 article, “False Positives, False Negatives, and False Analyses” (Flores et al. 2016). Although not peer-reviewed and somewhat hyperbolic in its language, this article has been widely cited and identified key limitations in Angwin et al.’s analysis. These included collapsing a spectrum of predicted risk into two categories, falsely equating racial differences in mean scores with bias in the tool (rather than systemic racial bias), neglecting to utilize standardized methods to test for bias, and overstating their results (39-40). Flores et al.’s own analysis found no evidence of racial bias in the COMPAS tool in terms of its accuracy in predicting recidivism rates (45). Only through publishing the original data were critiques like these made possible, thereby creating space for open debates on research methodologies, definitions of bias, and just uses of predictive modeling in criminal justice.\nSamples and their generalizability\nThe sample used to create the COMPAS tool was not made public, so it is therefore difficult to determine if the sample was representative. Therefore, follow-up studies like Angwin et al’s were needed to test for bias.Angwin et al. compiled a dataset of over 7,000 individuals arrested in Broward County, Florida, in 2013 and 2014, and determined whether they had committed new crimes in the subsequent two years (2016). The article’s text offers no specific reason for why Broward County was selected; however, according to the 2020 census, the county is home to significant populations of non-Hispanic whites (33.1%), non-Hispanic Blacks (26.6%), and Hispanics (31.3%). Because the purpose of the study was to analyze the bias of a predictive tool, it is perhaps less important that the sample population is closely representative of the US population as a whole. Nevertheless, the authors should explain why they selected Broward and why they chose to limit much of their analysis to white and Black populations. The authors were transparent about how they collected the data, and much (if not all) of the data was already publicly available via court documentation and can therefore be verified.\nConsent structures\nThe “participants” of both the COMPAS tool development and Angwin et al.’s study were neither aware that their data would be used for research nor able to provide informed consent. Criminal records are already publicly available, so I do not believe Angwin and her team violated any laws by conducting research with the data. However, by publishing the data on a highly popular and visible new article, they increased the probability that individuals in the defendants’ social networks will discover their criminal status, which the defendants may have not chosen to disclose otherwise.\nConclusion\nAngwin et al.’s results suggest that racial bias in risk assessment tools can harm Black people by causing them to serve longer sentences in jails and prisons, even when they may be at lower risk of recidivism (2016). Flores et al. point out that the existing justice system is already biased against poor minorities due to economic factors, policing practices, prosecutor behavior, and judicial biases (2016, 38). In their view, the status quo is already biased, and risk assessment tools well-screened for racial biases could help reduce personal biases and human errors that exacerbate current disparaties (38). I believe that both human and computer decision-making are subject to bias and must be scrutinized, made transparent, and held accountable. Risk assessment tools are particularly dangerous when they lack transparency and rigorous testing and are then used at scale to cut costs and save time for judicial proceedings.\nSources\nFlores, Anthony W, Kristin Bechtel, and Christopher Lowemkamp. 2016. “False Positives, False Negatives, and False Analyses: A Rejoinder to ‘Machine Bias: There’s Software Used Across the Country to Predict Future Criminals. And It’s Biased Against Blacks.’” Federal Probation 80: 2 https://heinonline.org/HOL/Page?handle=hein.journals/fedpro80&id=116&collection=journals&index=.\nAngwin Mattu, Julia, Jeff Larson, Lauren Kirchner, and Surya. 2016. “Machine Bias.” ProPublica, May 23. https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing."
  },
  {
    "objectID": "Project2.html",
    "href": "Project2.html",
    "title": "FLOTUS Tweets",
    "section": "",
    "text": "The following analysis of Michelle Obama’s tweets from 2013 to 2016 addresses three questions:\n\nWhat were her ten most common hash tags, and how many times did she use them?\nFrom which ten accounts did she re-tweet most frequently and how many times did she re-tweet for each?\nHow did her usage of words central to her policy platform—girls, health, and education—change from year to year?\n\nTo gain a general understanding of the Michelle Obama’s tweeting habits while in office, I graphed how much she tweeted over time:\n\n\nShow the code\nlibrary(tidyverse)\nlibrary(lubridate)\n\nflotus_tweets &lt;- read.csv(\"tweets.csv\")\n\nflotus_tweets_over_time &lt;- flotus_tweets |&gt;\n  mutate(year_month = floor_date(as_date(timestamp), \"month\")) |&gt;\n  group_by(year_month) |&gt;\n  summarize(count = n())\n\nggplot(data = flotus_tweets_over_time, \n       aes(x = year_month, y = count)) +\n  geom_line(color = \"pink\", size = 2) +\n  labs(x = \"Year\", \n       y = \"Number of Tweets\", \n       title = \"Michelle Obama's Tweeting History\") +\n  theme_minimal()\n\n\n\n\n\nAlthough there was significant month-to-month variation in tweeting volume, the First Lady’s tweeting generally increased through to 2015, reached a high of 250 tweets in August 2015, and began to taper off in 2016.\n\n\n\n\nAlthough there was significant month-to-month variation in tweeting volume, the First Lady’s tweeting generally increased through to 2015 and began to taper off in 2016. Data collection ended in October 2016.\nTo understand what campaigns Michelle Obama was promoting, I scraped the hash tags from her tweets (the word characters following #) and found the ten that were used most extensively.\n\n\nShow the code\n#Extracts hash tags\nflotus_hashtags &lt;- flotus_tweets |&gt;\n  select(tweet_id, text) |&gt;\n  mutate(hashtags = str_extract_all(text, \"(?&lt;=#)\\\\w+\"))\n\n#Unnests hash tags\nhashtags_unnested &lt;- flotus_hashtags |&gt;\n  unnest(hashtags)\n\n#Counts instances of each hash tag and reports top 10\nhashtag_counts &lt;- hashtags_unnested |&gt;\n  count(hashtags, sort = TRUE) |&gt;\n  arrange(desc(n)) |&gt;\n  head(10) |&gt;\n  mutate(hashtags = fct_reorder(hashtags, n))\n\n#Bar chart\nggplot(data=hashtag_counts, \n       aes(y=hashtags, x=n, fill=hashtags)) +\n  scale_fill_manual(values = c(rep(\"grey\", 9), \"pink\")) +\n  geom_col(show.legend = FALSE) + \n  labs(title = \"Michelle Obama's Top 10 Hashtags\", \n       y=\"Hashtag\", \n       x=\"Number of uses\") +\n  theme_minimal()\n\n\n\n\n\nMichelle Obama’s most commonly used hashtag was #LetGirlsLearn (397), followed by #ReachHigher (302), #GimmeFive (163), #62MillionGirls (160), and #LetsMove (153).\n\n\n\n\nAs you can see from the graph above, Michelle Obama’s most commonly used hashtag was #LetGirlsLearn, followed by #ReachHigher, #GimmeFive, #62MillionGirls, and #LetsMove. Although her use of hashtags likely does not map on perfectly to her priorities, it provides some indication of the social media campaigns she was promoting most actively.\nNext, I extracted her re-tweets by finding the words following @ to understand what accounts she was interacting with and amplifying on Twitter.\n\n\nShow the code\n#Extracts retweets by identifying words following @\nflotus_retweets &lt;- flotus_tweets|&gt;\n  select(tweet_id, timestamp, text) |&gt;\n  mutate(retweets = str_extract(text, \"(?&lt;=RT @)\\\\w+\")) |&gt;\n  filter(!is.na(retweets))\n\n#Unnests retweets\nretweets_unnested &lt;- flotus_retweets |&gt;\n  unnest(retweets)\n\n#Counts instances of retweets and reports top 10 most common\nretweet_counts &lt;- retweets_unnested |&gt;\n  count(retweets, sort = TRUE) |&gt;\n  arrange(desc(n)) |&gt;\n  head(10) |&gt;\n  mutate(retweets = fct_reorder(retweets, n))\n\nggplot(data=retweet_counts, aes(y=retweets, x=n, fill=retweets)) +\n  scale_fill_manual(values = c(rep(\"grey\", 9), \"light blue\")) +\n  geom_col(show.legend = FALSE) + \n  labs(title = \"Michelle Obama's Top 10 Accounts for Retweets\", \n       y=\"Accounts\", \n       x=\"Number of retweets\") +\n  theme_minimal()\n\n\n\n\n\nThe top accounts Obama retweeted were the Let’s Move Campaign (102 times), the White House (87 times), the Joining Forces Campaign (64 times), and the Reach Higher Campaign (48 times).\n\n\n\n\nMichelle Obama re-tweeted heavily from a number of accounts, especially the Let’s Move Campaign, the White House, the Joining Forces Campaign (an initiative with Jill Biden to support veterans), and the Reach Higher Campaign (for educational access). Clearly, she was promoting a variety of initiatives relevant to her platform as first lady.\nMichelle Obama used her platform as First Lady to promote education, health, and opportunities for girls. I wanted to understand how her usage of these terms on Twitter changed over the course of her time in the White House.\n\n\nShow the code\n#Creates three new columns representing whether each of the three terms were mentioned in the tweet\nflotus_topics &lt;- flotus_tweets|&gt;\n  select(tweet_id, timestamp, text) |&gt;\n  mutate(girls = str_detect(text, \"girls*\")) |&gt;\n  mutate(education = str_detect(text, \"education\")) |&gt;\n  mutate(health = str_detect(text, \"health\")) |&gt;\n  mutate(year = year(timestamp)) |&gt;\n  group_by(year, girls, education, health) |&gt;\n  group_by(year) |&gt;\n  summarize(across(c(girls, education, health), sum)) |&gt; #Finds total mentions of each category\n  pivot_longer(-year, names_to=\"Topics\", values_to=\"mentions\") #Pivots longer for ggplot\n\nggplot(data=flotus_topics, \n       aes(y=mentions, x=year, color=Topics)) +\n   geom_line() + \n   labs(title = \"Michelle Obama's Tweets on Girls, Education, and Health over Time\", \n        y=\"Tweets\", \n        x=\"Year\") +\n  theme_minimal()\n\n\n\n\n\nAlthough her total amount of tweets about girls, education, and health increased from 2013 to 2014 and from 2014 to 2015, her usage of the term ‘girl’ spiked particularly dramatically in 2015 (184 mentions). She used all three terms less frequently in 2016, when her Twitter usage generally was tapering off.\n\n\n\n\nAlthough her total amount of tweets including these three terms increased from 2013 to 2014 and from 2014 to 2015, her usage of the term “girl” spiked particularly dramatically in 2015. She used all three terms less frequently in 2016, when her Twitter usage generally was tapering off.\nData source: “Archived White House Websites and Social Media.” 2017. Barack Obama Presidential Library. Accessed October 4, 2025. https://www.obamalibrary.gov/digital-research-room/archived-white-house-websites-and-social-media."
  },
  {
    "objectID": "Project3.html",
    "href": "Project3.html",
    "title": "Gender Representation in the US Congress",
    "section": "",
    "text": "Show the code\nlibrary(reprex)\nset.seed(47)\nlibrary(tidyverse)\nlibrary(readr)\n\n#Reads data\nurl &lt;- \"https://raw.githubusercontent.com/unitedstates/congress-legislators/gh-pages/legislators-current.csv\"\ncurrent_legislators &lt;- read_csv(url)\n\n#Adds numbers of men and women in each party\nrep_and_dem_women &lt;- current_legislators |&gt; \n  group_by(party, gender) |&gt;\n  summarize(count = n(), .groups = \"drop\") |&gt;\n  group_by(party) |&gt;\n  mutate(proportion_gender_by_party = count/sum(count)) |&gt;\n  filter(party == \"Democrat\" | party == \"Republican\")\n\n#Creates segmented bar chart\nggplot(rep_and_dem_women, aes(x = party, y = count, fill = gender)) +\n      geom_bar(stat = \"identity\", position = \"stack\") +\n      labs(title = \"Members of the US Congress by Party and Gender\",\n           x = \"Party\",\n           y = \"Number of Members\",\n           fill = \"Gender\") +\n      theme_minimal()\n\n\n\n\n\nThe distribution of women across parties is currently uneven, with 113 in the Democratic Party, and the other 43 in the Republican Party.\n\n\n\n\nThere are currently 156 female members in the US Congress, out of 538 members. The distribution of women across parties is uneven: 113 are part of the Democratic Party, and the other 43 are part of the Republican Party. Assuming that women are equally likely to be a part of each party, how likely is it that number of Democratic women is 113 are more? How likely is it that different levels of female representation is related to the party rather than random chance? These questions lead to the following null and alternative hypotheses:\nH0: The process that leads to whether a congressperson is female or male has nothing to do with political party. Therefore, the expected proportion of congresspeople who are women would be the same across political parties.\nHA: The process that leads to whether a congressperson is female or male is related to political party. Therefore, the expected proportion of congresswomen would be different across political party.\nTo test these hypotheses, I first wrangled data on the current members of Congress to calculate the numbers of men and women in each of the two main parties. Next, I created a data frame with the same numbers of Democrats, Republicans, women, and men, and then created a function that randomly assigned parties to the men and women in the data frame and then calculates the total number of Democratic women. Finally, I ran this function 5000 times using the map_dbl function and then calculated the number of times that Democratic women exceeded 113, divided by the number of iterations (5000). The figure below shows the distribution of the numbers of congresswomen in the Democratic Parity when the assignment of women to each party was random.\n\n\nShow the code\n#Creates starting data with current distribution of gender\nstart_data &lt;- data.frame(gender = c(rep(\"F\", 156), rep(\"M\", 382)),\n                         party = c(rep(\"Democrat\", 262), rep(\"Republican\", 276)))\n\n#Permutes party identification across male and female members of congress and finds the sum of Democratic women.\nrandom_female_democrats &lt;- function(rep) {\n  start_data |&gt; mutate(perm_party = sample(party)) |&gt;\n    group_by(gender, perm_party) |&gt;\n    summarize(count = n(), .groups = \"drop\") |&gt;\n    filter(perm_party == \"Democrat\", gender == \"F\") |&gt;\n    select(count) |&gt;\n    pull()\n}\n\n#Repeats permutation 5000 times, producing data frame with number of democratic women for each repetition.\nnum_exper &lt;- 5000\nfemale_democrats &lt;- map_dbl(1:num_exper, random_female_democrats)\n\n#Plots distribution of random permutations\nfemale_democrats |&gt;\n   data.frame() |&gt;\n   ggplot(aes(x = female_democrats)) +\n   geom_histogram() +\n   geom_vline(xintercept = 113, color = \"red\") +\n   labs(x = \"Number of women in Democratic seats in Congress\",\n        title = \"Numbers of Democratic women when party assignments are random\",\n        subtitle = \"Higher female representation in the Democratic party is unlikely due to random chance.\") +\n  theme_minimal()\n\n\n\n\n\nThe distribution is centered at 78, and none of the simulations reached or exceeped the current number of 113 women in the Democratic Party.\n\n\n\n\nAs expected, the random distribution of Democratic women centers around 78, or half of the total women in Congress. The actual number of Democratic women in congressional seats far exceeds the number of women in Democratic seats when the assignment of party to women and men is random.\nNext, I calculated the p-value, denoting the probability that the number of Democratic congresswomen exceeded 113 if the party assignment of men and women were a random process repeated 5000 times.\n\n\nShow the code\n#Calculates p-value\ndata.frame(p_value = sum(female_democrats &gt;= 113) / num_exper)\n\n\n  p_value\n1       0\n\n\nI calculated that when women are randomly assigned to Democratic and Republican seats 5000 times, the probability that 113 or more were in the Democratic Party was 0. This result means that we can reject the null hypothesis, providing evidence that the higher number of women in the Democratic Party, in comparison to the Republican Party, has something to do with the parties rather than random chance. Future research should determine the specific mechanisms facilitating greater female participation in the Democratic Party.\nData source: Zachary Kornbluth. 2025. Unitedstates/Congress-Legislators. Python. December 6, Released October 22, 2025. https://github.com/unitedstates/congress-legislators.\nOriginal source: GovTrack.US. n.d. “List of Representatives and Senators.” Accessed December 6, 2025. https://www.govtrack.us/congress/members/current"
  }
]