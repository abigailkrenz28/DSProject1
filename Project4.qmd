---
title: "Ethical Dilemmas of Risk Assessment Tools in Criminal Justice"
description: |
  
author: Abigail Krenz
date: December 6, 2025
format: 
  html:
    code-fold: true
    code-summary: "Show the code"
---

In a 2016 ProPublica article, Julia Angwin and her co-authors revealed that Northpointe's COMPAS, a prominent risk assessment tool used to inform decisions in the criminal justice system, was racially biased. While the tool failed at similar rates for white and Black defendants, it falsely labeled 44.9% of Black defendants as high risk who did not re-offend, as compared to 23.5% of white defendants (2016). Actuarial risk assessment instruments (ARAIs) like COMPAS are increasingly being used across the criminal justice system. Its proponents argue they could reduce existing racial biases and ultimately decrease incarceration rates (Flores et al. 2016, 38), while detractors argue these untested and non-transparent tools risk amplifying biases against people of color. 

**Use of race as a variable in COMPAS tool and follow-up studies**

Using race as a category is morally unacceptable in developing a risk assessment tool for use in the criminal justice system. However, even though race was not used in the COMPAS tool, certain metrics such as "substance abuse" and "residence/stability" might be correlated with racial categories, leading to bias in the model. For this reason, employing race as a variable is necessary for independent evaluations of tools like COMPAS. Only by documenting defendants' race could Angwin and her team identify bias and ultimately pressure courts to reconsider their use of the tool. 

**Data transparency in COMPAS and follow-up studies**

The COMPAS algorithm itself and the data used to create it were not made public. In their follow-up investigation, however, Angwin and her team published their data on GitHub and added a link to the data to their ProPublica article. This transparency allowed other scholars to verify and critique their findings, as evidenced by the 2016 article, "False Positives, False Negatives, and False Analyses" (Flores et al. 2016). Although not peer-reviewed and somewhat hyperbolic in its language, this article has been widely cited and identified key limitations in Angwin et al.'s analysis. These included collapsing a spectrum of predicted risk into two categories, falsely equating racial differences in mean scores with bias in the tool (rather than systemic racial bias), neglecting to utilize standardized methods to test for bias, and overstating their results (39-40). Flores et al.'s own analysis found no evidence of racial bias in the COMPAS tool in terms of its accuracy in predicting recidivism rates (45). Only through publishing the original data were critiques like these made possible, thereby creating space for open debates on research methodologies, definitions of bias, and just uses of predictive modeling in criminal justice. 

**Samples and their generalizability**

The sample used to create the COMPAS tool was not made public, so it is therefore difficult to determine if the sample was representative. Therefore, follow-up studies like Angwin et al's were needed to test for bias.Angwin et al. compiled a dataset of over 7,000 individuals arrested in Broward County, Florida, in 2013 and 2014, and determined whether they had committed new crimes in the subsequent two years (2016). The article's text offers no specific reason for why Broward County was selected; however, according to the 2020 census, the county is home to significant populations of non-Hispanic whites (33.1%), non-Hispanic Blacks (26.6%), and Hispanics (31.3%). Because the purpose of the study was to analyze the bias of a predictive tool, it is perhaps less important that the sample population is closely representative of the US population as a whole. Nevertheless, the authors should explain why they selected Broward and why they chose to limit much of their analysis to white and Black populations. The authors were transparent about how they collected the data, and much (if not all) of the data was already publicly available via court documentation and can therefore be verified. 

**Consent structures**

The "participants" of both the COMPAS tool development and Angwin et al.'s study were neither aware that their data would be used for research nor able to provide informed consent. Criminal records are already publicly available, so I do not believe Angwin and her team violated any laws by conducting research with the data. However, by publishing the data on a highly popular and visible new article, they increased the probability that individuals in the defendants' social networks will discover their criminal status, which the defendants may have not chosen to disclose otherwise. 

**Conclusion**

Angwin et al.'s results suggest that racial bias in risk assessment tools can harm Black people by causing them to serve longer sentences in jails and prisons, even when they may be at lower risk of recidivism (2016). Flores et al. point out that the existing justice system is already biased against poor minorities due to economic factors, policing practices, prosecutor behavior, and judicial biases (2016, 38). In their view, the status quo is already biased, and risk assessment tools well-screened for racial biases could help reduce personal biases and human errors that exacerbate current disparaties (38). I believe that both human and computer decision-making are subject to bias and must be scrutinized, made transparent, and held accountable. Risk assessment tools are particularly dangerous when they lack transparency and rigorous testing and are then used at scale to cut costs and save time for judicial proceedings.

**Sources**

Flores, Anthony W, Kristin Bechtel, and Christopher Lowemkamp. 2016. "False Positives, False Negatives, and False Analyses: A Rejoinder to 'Machine Bias: There’s Software Used Across the Country to Predict Future Criminals. And It’s Biased Against Blacks.'" Federal Probation 80: 2 [https://heinonline.org/HOL/Page?handle=hein.journals/fedpro80&id=116&collection=journals&index=](https://heinonline.org/HOL/Page?handle=hein.journals/fedpro80&id=116&collection=journals&index=).

Angwin Mattu, Julia, Jeff Larson, Lauren Kirchner, and Surya. 2016. “Machine Bias.” ProPublica, May 23. [https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing).





