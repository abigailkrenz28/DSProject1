---
title: "Ethical Dilemmas of Risk Assessment Tools in Criminal Justice"
description: |
  
author: Abigail Krenz
date: November 11, 2025
format: 
  html:
    code-fold: true
    code-summary: "Show the code"
---

In a 2016 ProPublica article, Julia Angwin and her co-authors revealed that Northpointe's COMPAS, a prominent risk assessment tool used to inform decisions in the criminal justice system, was racially biased. The tool failed at similar rates for white and Black defendants. However, it falsely labeled 44.9% of Black defendants as high risk who did not re-offend, compared to 23.5% of white defendants (2016). Actuarial risk assessment instruments (ARAIs) like COMPAS are increasingly being used across the criminal justice system. Its proponents argue they could reduce existing racial biases and ultimately decrease incarceration rates (Flores et al. 2016, 38), while detractors argue these untested and non-transparent tools risk amplifying biases against people of color. 

1. Should race be used as a variable?

Using race as a category is morally unacceptable in developing a risk assessment tool for use in the criminal justice system. However, even though race was not used in the COMPAS tool, certain metrics such as "substance abuse" and "residence/stability" might be correlated with racial categories, leading to bias in the model. For this reason, employing race as a variable is necessary for independent evaluations of tools like COMPAS. Only by documenting defendants' race could Angwin and her team identify bias and ultimately pressure courts to reconsider their use of the tool. 

2. Were the data made publicly available? Why? How? On what platform?

Yes, the data were made publicly available on GitHub and are directly linked to the ProPublica article. This transparency allowed other scholars to verify and critique their findings, as evidenced by the 2016 article, "False Positives, False Negatives, and False Analyses." Although not peer-reviewed and somewhat hyperbolic in its language, the article has been widely cited and identified key limitations in Angwin et al.'s analysis. These included collapsing a spectrum of predicted risk into two categories, falsely equating racial differences in mean scores with bias in the tool (rather than systemic racial bias), neglecting to utilize standardized methods to test for bias, and overstating their results (Flores et al. 2016, 39-40). Their own analysis found no evidence of racial bias in the COMPAS tool in terms of its accuracy in predicting recidivism rates (45). Only through publishing the original data were critiques like this made possible, thereby creating space for open debates over research methodologies, definitions of bias, and just uses of predictive modeling in criminal justice. 

3. Who was measured? Are those individuals representative of the people to whom we’d like to generalize / apply the algorithm? Should we analyze data if we do not know how the data were collected?

Angwin et al. compiled a dataset of over 7,000 individuals arrested in Broward County, Florida, in 2013 and 2014, and determined whether they had committed new crimes in the subsequent two years (2016). The article's text offers no specific reason for why Broward County was selected; however, according to the 2020 census, the county is home to significant populations of non-Hispanic whites (33.1%), non-Hispanic Blacks (26.6%), and Hispanics (31.3%). Because the purpose of the study was to analyze the bias of a predictive tool, it is perhaps less important that the sample population is closely representative of the US population as a whole. Nevertheless, the authors should explain why they selected Broward and why they chose to limit much of their analysis to white and Black populations. The authors were transparent about how they collected the data, and much (if not all) of the data is publicly available via court documentation and can therefore be verified. 

4. What was the consent structure for recruiting participants? Were the participants aware of the ways their data would be used for research? Was informed consent possible? Can you provide informed consent for applications that are yet foreseen?

The "participants" of the study were neither aware that their data would be used for research nor able to provide informed consent. Criminal records are already publicly available, and I don't think the researchers violated any laws by conducting research using the data. However, by publishing the data on a highly popular and visible new article, they increase the probability that individuals in the defendants' social networks will discover their criminal status, which the defendants would not have otherwise chosen to disclose. 

Angwin et al.'s results suggest that racial bias in risk assessment tools can harm Black people by causing them to serve longer sentences in jails and prisons even when they may be at lower risk of recidivism (2016). Flores et al. point out that the existing justice system is already biased against poor minorities due to economic factors, policing practices, prosecutor behavior, and judicial biases (2016, 38). In their view, the status quo is already biased, and risk assessment tools well-screened for racial biases could help reduce personal biases and human errors (38). I believe that both human and computer decision-making are subject to bias and must be scrutinized, made transparent, and held accountable. Risk assessment tools are particularly dangerous when they lack both transparency and rigorous testing and are then used at scale to cut costs and save time for judicial proceedings.

Sources: 

Flores, Anthony W, Kristin Bechtel, and Christopher Lowemkamp. 2016. "False Positives, False Negatives, and False Analyses: A Rejoinder to 'Machine Bias: There’s Software Used Across the Country to Predict Future Criminals. And It’s Biased Against Blacks.'" Federal Probation 80: 2 [https://heinonline.org/HOL/Page?handle=hein.journals/fedpro80&id=116&collection=journals&index=](https://heinonline.org/HOL/Page?handle=hein.journals/fedpro80&id=116&collection=journals&index=).

Angwin Mattu, Julia, Jeff Larson, Lauren Kirchner, and Surya. 2016. “Machine Bias.” ProPublica, May 23. [https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing).





